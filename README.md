# Activation Function Research Project

NghiÃªn cá»©u so sÃ¡nh hiá»‡u nÄƒng cá»§a cÃ¡c activation functions trÃªn cÃ¡c neural network architectures khÃ¡c nhau.

## ğŸ“‹ Tá»•ng quan

Project nÃ y cho phÃ©p báº¡n:
- So sÃ¡nh nhiá»u activation functions (ReLU, Sigmoid, Tanh, Swish, GELU, Mish, v.v.)
- Thá»­ nghiá»‡m trÃªn nhiá»u model architectures (AlexNet, VGG16, ResNet, EfficientNet)
- ÄÃ¡nh giÃ¡ trÃªn cÃ¡c datasets chuáº©n (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100)
- Tá»± Ä‘á»™ng táº¡o visualizations vÃ  bÃ¡o cÃ¡o chi tiáº¿t

## ğŸ—‚ï¸ Cáº¥u trÃºc Project

```
activation-function-research/
â”‚
â”œâ”€â”€ data/                       # Data loading
â”‚   â””â”€â”€ data_loader.py
â”‚
â”œâ”€â”€ models/                     # Model architectures
â”‚   â”œâ”€â”€ base_model.py
â”‚   â”œâ”€â”€ alexnet.py
â”‚   â”œâ”€â”€ vgg16.py
â”‚   â”œâ”€â”€ resnet.py
â”‚   â””â”€â”€ efficientnet.py
â”‚
â”œâ”€â”€ activation_functions/       # Activation functions
â”‚   â””â”€â”€ activations.py
â”‚
â”œâ”€â”€ training/                   # Training utilities
â”‚   â””â”€â”€ trainer.py
â”‚
â”œâ”€â”€ evaluation/                 # Evaluation & visualization
â”‚   â””â”€â”€ visualization.py
â”‚
â”œâ”€â”€ experiments/                # Experiment runner
â”‚   â”œâ”€â”€ run_experiment.py
â”‚   â””â”€â”€ experiment_config.yaml
â”‚
â”œâ”€â”€ results/                    # Output directory
â”‚   â”œâ”€â”€ models/                 # Saved models
â”‚   â”œâ”€â”€ logs/                   # Training logs
â”‚   â””â”€â”€ plots/                  # Visualizations
â”‚
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â”‚   â””â”€â”€ analysis.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â””â”€â”€ README.md
```

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository hoáº·c táº¡o thÆ° má»¥c project

```bash
mkdir activation-function-research
cd activation-function-research
```

### 2. Táº¡o virtual environment

```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# hoáº·c
venv\Scripts\activate     # Windows
```

### 3. CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

## ğŸ’» Sá»­ dá»¥ng

### Quick Start - Test nhanh

```bash
# Test nhanh vá»›i 1 epoch
python main.py --quick-test
```

### Cháº¡y táº¥t cáº£ experiments

```bash
# Cháº¡y táº¥t cáº£ cÃ¡c thÃ­ nghiá»‡m trong config
python main.py
```

### Cháº¡y experiment cá»¥ thá»ƒ

```bash
# Chá»‰ Ä‘á»‹nh dataset, model vÃ  activation
python main.py --dataset mnist --model alexnet_small --activation relu
```

### Sá»­ dá»¥ng custom config

```bash
# DÃ¹ng file config tÃ¹y chá»‰nh
python main.py --config my_experiment_config.yaml
```

### Táº¯t GPU

```bash
# Cháº¡y trÃªn CPU
python main.py --no-gpu
```

## âš™ï¸ Configuration

Chá»‰nh sá»­a file `experiments/experiment_config.yaml`:

```yaml
# Training parameters
epochs: 50
batch_size: 128
learning_rate: 0.001
optimizer: 'adam'

# Datasets
datasets:
  - 'mnist'
  - 'fashion_mnist'
  - 'cifar10'

# Models
models:
  - 'alexnet_small'
  - 'vgg16_small'
  - 'resnet18'
  - 'efficientnet_small'

# Activation functions
activations:
  - 'relu'
  - 'sigmoid'
  - 'tanh'
  - 'swish'
  - 'gelu'
  - 'mish'
```

## ğŸ¯ ThÃªm Activation Function má»›i

### 1. ThÃªm vÃ o `activation_functions/activations.py`

```python
@tf.function
def my_new_activation(x):
    """
    Your custom activation function
    
    Example: f(x) = x * sigmoid(x) * tanh(x)
    """
    return x * tf.nn.sigmoid(x) * tf.nn.tanh(x)

# ThÃªm vÃ o dictionary
ACTIVATION_FUNCTIONS = {
    # ... existing activations
    'my_new_activation': 'my_new_activation'
}

# ThÃªm vÃ o get_activation function
def get_activation(name):
    # ...
    elif name == 'my_new_activation':
        return my_new_activation
```

### 2. ThÃªm vÃ o config file

```yaml
activations:
  - 'relu'
  - 'my_new_activation'  # Add your new activation
```

### 3. Cháº¡y experiments

```bash
python main.py --activation my_new_activation --dataset mnist --model resnet18
```

## ğŸ“Š Káº¿t quáº£ vÃ  Visualizations

Sau khi cháº¡y experiments, káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `results/`:

### Files Ä‘Æ°á»£c táº¡o:

1. **CSV Results**: `results/all_results.csv`
   - Báº£ng tá»•ng há»£p táº¥t cáº£ cÃ¡c metrics

2. **Model Checkpoints**: `results/models/`
   - Best models cho má»—i experiment

3. **Training Logs**: `results/logs/`
   - TensorBoard logs
   - CSV training history

4. **Visualizations**: `results/plots/`
   - Training history plots
   - Comparison charts
   - Heatmaps
   - Comprehensive report

### Xem TensorBoard

```bash
tensorboard --logdir results/logs
```

## ğŸ“ˆ Metrics Ä‘Æ°á»£c tracking

- **Accuracy**: Test accuracy
- **Top-5 Accuracy**: Top-5 categorical accuracy
- **Loss**: Categorical cross-entropy loss
- **Precision & Recall**: Classification metrics
- **Training Time**: Thá»i gian training
- **Model Parameters**: Sá»‘ lÆ°á»£ng parameters

## ğŸ”¬ CÃ¡c Models cÃ³ sáºµn

1. **AlexNet** (`alexnet`, `alexnet_small`)
   - Classic CNN architecture
   - Adapted cho small images

2. **VGG16** (`vgg16`, `vgg16_small`)
   - Deep architecture vá»›i 16 layers
   - Smaller version cho faster training

3. **ResNet** (`resnet18`, `resnet34`, `resnet50`)
   - Residual connections
   - CÃ¡c Ä‘á»™ sÃ¢u khÃ¡c nhau

4. **EfficientNet** (`efficientnet_small`)
   - Modern efficient architecture
   - Mobile Inverted Bottleneck blocks

## ğŸ¨ Activation Functions

### Standard Activations:
- **ReLU**: Rectified Linear Unit
- **Sigmoid**: Logistic function
- **Tanh**: Hyperbolic tangent
- **Softmax**: Multi-class output

### Advanced Activations:
- **Swish**: Self-gated activation
- **GELU**: Gaussian Error Linear Unit
- **Mish**: x * tanh(softplus(x))
- **ELU**: Exponential Linear Unit
- **SELU**: Scaled ELU

### Custom Activations:
- ThÃªm cá»§a riÃªng báº¡n!

## ğŸ“ VÃ­ dá»¥ vá» káº¿t quáº£

```
EXPERIMENT SUMMARY
================================================================================

1. Best Overall Performance:
   Experiment: mnist_resnet18_gelu
   Test Accuracy: 0.9934
   Model: resnet18
   Activation: gelu
   Dataset: mnist

2. Best Activation Function (Average):
   gelu: 0.9847
   swish: 0.9831
   mish: 0.9819

3. Best Model Architecture (Average):
   resnet18: 0.9856
   efficientnet_small: 0.9823
   vgg16_small: 0.9801
```

## ğŸ› ï¸ Troubleshooting

### Out of Memory (OOM)

```bash
# Giáº£m batch size trong config
batch_size: 64  # thay vÃ¬ 128

# Hoáº·c dÃ¹ng smaller models
models:
  - 'alexnet_small'
  - 'vgg16_small'
```

### Slow Training

```bash
# Giáº£m sá»‘ epochs
epochs: 20

# DÃ¹ng fewer combinations
datasets:
  - 'mnist'  # only one dataset
```

### GPU khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng

```bash
# Check GPU availability
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

## ğŸ“š TÃ i liá»‡u tham kháº£o

1. **AlexNet**: [ImageNet Classification with Deep CNNs](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)

2. **VGG**: [Very Deep Convolutional Networks](https://arxiv.org/abs/1409.1556)

3. **ResNet**: [Deep Residual Learning](https://arxiv.org/abs/1512.03385)

4. **EfficientNet**: [EfficientNet: Rethinking Model Scaling](https://arxiv.org/abs/1905.11946)

5. **Swish**: [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)

6. **GELU**: [Gaussian Error Linear Units](https://arxiv.org/abs/1606.08415)

7. **Mish**: [Mish: A Self Regularized Non-Monotonic Activation](https://arxiv.org/abs/1908.08681)

## ğŸ¤ Contributing

ÄÃ³ng gÃ³p cá»§a báº¡n luÃ´n Ä‘Æ°á»£c hoan nghÃªnh! Má»™t sá»‘ Ã½ tÆ°á»Ÿng:

1. ThÃªm activation functions má»›i
2. Implement thÃªm model architectures
3. ThÃªm datasets má»›i
4. Cáº£i thiá»‡n visualizations
5. Optimize training speed

## ğŸ“„ License

MIT License - feel free to use for research and education!

## ğŸ‘¨â€ğŸ’» Author

Your Name - Activation Function Research Project

## ğŸ™ Acknowledgments

- TensorFlow team
- Keras team
- OpenAI & Anthropic for AI assistance
- Research community for papers and implementations

---

**Happy Researching! ğŸš€**